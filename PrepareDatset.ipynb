{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4889efb5",
   "metadata": {},
   "source": [
    "Preparación y extracción de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "933e6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importacion de librerias\n",
    "\n",
    "from scapy.all import rdpcap, TCP, IP\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d8e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset/onu\\ONU_capture_game_1.pcapng → clase game\n",
      "Procesando Dataset/onu\\ONU_capture_game_2.pcapng → clase game\n",
      "Procesando Dataset/onu\\ONU_capture_game_3.pcapng → clase game\n",
      "Procesando Dataset/onu\\ONU_capture_game_4.pcapng → clase game\n",
      "Procesando Dataset/onu\\ONU_capture_game_5.pcapng → clase game\n",
      "Procesando Dataset/onu\\ONU_capture_game_6.pcapng → clase game\n",
      "✅ Guardado 5357 flujos en Dataset/clases1/game_simple.csv\n"
     ]
    }
   ],
   "source": [
    "# Pruebas con extracción simple de características de flujos de red a partir de archivos PCAP\n",
    "\n",
    "# Patrón de archivos PCAP a procesar\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_game_[1-6].pcapng\"  \n",
    "# Archivo CSV de salida donde se guardarán los datos procesados\n",
    "output_csv = \"Dataset/clases1/game.csv\"\n",
    "# Clase o etiqueta que se asignará a estos flujos\n",
    "target_class = \"game\"\n",
    "\n",
    "# Función que procesa un archivo PCAP y extrae características simples de cada flujo\n",
    "def process_pcap(pcap_file, label):\n",
    "    # Cargar todos los paquetes del archivo PCAP\n",
    "    packets = rdpcap(pcap_file)\n",
    "    \n",
    "    # Diccionario para almacenar información de cada flujo, inicializando valores por defecto\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0,      # Número total de paquetes en el flujo\n",
    "        \"bytes\": 0,        # Número total de bytes en el flujo\n",
    "        \"times\": [],       # Tiempos de llegada de cada paquete\n",
    "        \"syn_count\": 0,    # Contador de paquetes TCP SYN\n",
    "        \"ack_count\": 0,    # Contador de paquetes TCP ACK\n",
    "        \"fin_count\": 0,    # Contador de paquetes TCP FIN\n",
    "        \"rst_count\": 0,    # Contador de paquetes TCP RST\n",
    "        \"pkt_sizes\": []    # Tamaños de cada paquete\n",
    "    })\n",
    "\n",
    "    # Recorrer cada paquete del archivo\n",
    "    for pkt in packets:\n",
    "        # Verificar si el paquete tiene capa IP\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst          # IP origen y destino\n",
    "            proto = pkt[IP].proto                        # Protocolo (TCP, UDP, etc.)\n",
    "            # Obtener puertos de origen y destino si existen, sino 0\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            # Identificador único de flujo basado en IPs, puertos y protocolo\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            # Actualizar estadísticas del flujo\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            # Si es TCP, contar flags específicos\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1  # SYN\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1  # ACK\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1  # FIN\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1  # RST\n",
    "\n",
    "    # Lista final para guardar los flujos procesados como diccionarios\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        # Calcular duración del flujo: diferencia entre el último y primer paquete\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        # Calcular tamaño promedio de paquete\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "\n",
    "        # Guardar todas las características en un diccionario\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO PRINCIPAL ===\n",
    "all_data = []  # Lista que almacenará todos los flujos de todos los archivos\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))  # Listar todos los archivos PCAP que coinciden con el patrón\n",
    "\n",
    "# Procesar cada archivo PCAP\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))  # Añadir flujos procesados a la lista principal\n",
    "\n",
    "# Guardar los datos procesados en un archivo CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)  # Crear directorio si no existe\n",
    "df = pd.DataFrame(all_data)  # Convertir lista de diccionarios a DataFrame\n",
    "df.to_csv(output_csv, index=False)  # Guardar CSV sin índice\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa40605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset/onu\\ONU_capture_instant-message_1.pcapng → clase instant-message\n",
      "Procesando Dataset/onu\\ONU_capture_instant-message_2.pcapng → clase instant-message\n",
      "Procesando Dataset/onu\\ONU_capture_instant-message_3.pcapng → clase instant-message\n",
      "✅ Guardado 4377 flujos en Dataset/clases1/instant-message.csv\n"
     ]
    }
   ],
   "source": [
    "# clase instant-message\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_instant-message_[1-3].pcapng\"   \n",
    "output_csv = \"Dataset/clases1/instant-message.csv\"\n",
    "target_class = \"instant-message\"\n",
    "\n",
    "def process_pcap(pcap_file, label):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0, \"bytes\": 0, \"times\": [],\n",
    "        \"syn_count\": 0, \"ack_count\": 0, \"fin_count\": 0, \"rst_count\": 0,\n",
    "        \"pkt_sizes\": []\n",
    "    })\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst\n",
    "            proto = pkt[IP].proto\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1\n",
    "\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO ===\n",
    "all_data = []\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))\n",
    "\n",
    "# Guardar CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23ad2b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset/onu\\ONU_capture_mail-service_1.pcapng → clase mail\n",
      "Procesando Dataset/onu\\ONU_capture_mail-service_2.pcapng → clase mail\n",
      "✅ Guardado 3045 flujos en Dataset/clases1/mail.csv\n"
     ]
    }
   ],
   "source": [
    "# clase mail\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_mail-service_[1-2].pcapng\"  \n",
    "output_csv = \"Dataset/clases1/mail.csv\"\n",
    "target_class = \"mail\"\n",
    "\n",
    "def process_pcap(pcap_file, label):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0, \"bytes\": 0, \"times\": [],\n",
    "        \"syn_count\": 0, \"ack_count\": 0, \"fin_count\": 0, \"rst_count\": 0,\n",
    "        \"pkt_sizes\": []\n",
    "    })\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst\n",
    "            proto = pkt[IP].proto\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1\n",
    "\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO ===\n",
    "all_data = []\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))\n",
    "\n",
    "# Guardar CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab6d7ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset/onu\\ONU_capture_network-storage_1.pcapng → clase network-storage\n",
      "Procesando Dataset/onu\\ONU_capture_network-storage_2.pcapng → clase network-storage\n",
      "Procesando Dataset/onu\\ONU_capture_network-storage_3.pcapng → clase network-storage\n",
      "✅ Guardado 4110 flujos en Dataset/clases1/network-storage.csv\n"
     ]
    }
   ],
   "source": [
    "# clase network-storage\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_network-storage_[1-3].pcapng\" \n",
    "output_csv = \"Dataset/clases1/network-storage.csv\"\n",
    "target_class = \"network-storage\"\n",
    "\n",
    "def process_pcap(pcap_file, label):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0, \"bytes\": 0, \"times\": [],\n",
    "        \"syn_count\": 0, \"ack_count\": 0, \"fin_count\": 0, \"rst_count\": 0,\n",
    "        \"pkt_sizes\": []\n",
    "    })\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst\n",
    "            proto = pkt[IP].proto\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1\n",
    "\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO ===\n",
    "all_data = []\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))\n",
    "\n",
    "# Guardar CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf495a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset/onu/ONU_capture_video_1.pcapng → clase video\n",
      "✅ Guardado 3131 flujos en Dataset/clases1/video.csv\n"
     ]
    }
   ],
   "source": [
    "# clase video\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_video_1.pcapng\" \n",
    "output_csv = \"Dataset/clases1/video.csv\"\n",
    "target_class = \"video\"\n",
    "\n",
    "def process_pcap(pcap_file, label):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0, \"bytes\": 0, \"times\": [],\n",
    "        \"syn_count\": 0, \"ack_count\": 0, \"fin_count\": 0, \"rst_count\": 0,\n",
    "        \"pkt_sizes\": []\n",
    "    })\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst\n",
    "            proto = pkt[IP].proto\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1\n",
    "\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO ===\n",
    "all_data = []\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))\n",
    "\n",
    "# Guardar CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b256b637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset/onu\\ONU_capture_web-browsing_1.pcapng → clase web-browsing\n",
      "Procesando Dataset/onu\\ONU_capture_web-browsing_2.pcapng → clase web-browsing\n",
      "✅ Guardado 4813 flujos en Dataset/clases1/web-browsing.csv\n"
     ]
    }
   ],
   "source": [
    "# web-browsing\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_web-browsing_[1-2].pcapng\" \n",
    "output_csv = \"Dataset/clases1/web-browsing.csv\"\n",
    "target_class = \"web-browsing\"\n",
    "\n",
    "def process_pcap(pcap_file, label):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0, \"bytes\": 0, \"times\": [],\n",
    "        \"syn_count\": 0, \"ack_count\": 0, \"fin_count\": 0, \"rst_count\": 0,\n",
    "        \"pkt_sizes\": []\n",
    "    })\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst\n",
    "            proto = pkt[IP].proto\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1\n",
    "\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO ===\n",
    "all_data = []\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))\n",
    "\n",
    "# Guardar CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c289332",
   "metadata": {},
   "source": [
    "Extración aplicando Ventana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f85e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Encontrados 4 archivos para procesar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Procesado ONU_capture_game_1.pcapng con 258 ventanas\n",
      "✅ Procesado ONU_capture_game_2.pcapng con 295 ventanas\n",
      "✅ Procesado ONU_capture_game_3.pcapng con 300 ventanas\n",
      "✅ Procesado ONU_capture_game_4.pcapng con 301 ventanas\n",
      "\n",
      "✅ Dataset guardado en Dataset/clases/game.csv con 1154 flujos\n",
      "        window_start         window_end  packets   bytes  duration  \\\n",
      "0  1693900141.261672  1693900142.261672        1     144  1.000000   \n",
      "1  1693900142.261672  1693900143.261672       18    5082  1.000000   \n",
      "2  1693900143.261672  1693900144.261672      576  314467  1.000000   \n",
      "3  1693900144.261672  1693900145.261672     1202  652591  1.000000   \n",
      "4  1693900145.261672  1693900146.261672        2     140  1.000000   \n",
      "\n",
      "   avg_pkt_size throughput pkt_rate class  \n",
      "0    144.000000        144        1  game  \n",
      "1    282.333333       5082       18  game  \n",
      "2    545.949653     314467      576  game  \n",
      "3    542.920965     652591     1202  game  \n",
      "4     70.000000     1.4E+2        2  game  \n"
     ]
    }
   ],
   "source": [
    "# clase game\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_game_[1-4].pcapng\"  # procesa 1 a 4\n",
    "output_csv = \"Dataset/clases2/game.csv\"\n",
    "target_class = \"game\"\n",
    "window_size = 1.0  # segundos\n",
    "\n",
    "# ==============================\n",
    "# Función para extraer features por ventanas\n",
    "# ==============================\n",
    "def extract_features_from_pcap_windowed(pcap_file, window_size=1.0):\n",
    "    packets = rdpcap(pcap_file)\n",
    "\n",
    "    if len(packets) == 0:\n",
    "        return []\n",
    "\n",
    "    start_time = packets[0].time\n",
    "    end_time = packets[-1].time\n",
    "    total_duration = end_time - start_time\n",
    "\n",
    "    n_windows = math.ceil(total_duration / window_size)\n",
    "    windows = [[] for _ in range(n_windows)]\n",
    "\n",
    "    # Distribuir paquetes en ventanas\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            ts = pkt.time\n",
    "            size = len(pkt)\n",
    "            win_idx = int((ts - start_time) // window_size)\n",
    "            if 0 <= win_idx < n_windows:\n",
    "                windows[win_idx].append(size)\n",
    "\n",
    "    # Extraer features de cada ventana\n",
    "    features = []\n",
    "    for i, pkts in enumerate(windows):\n",
    "        if len(pkts) == 0:\n",
    "            continue\n",
    "\n",
    "        window_start = start_time + i * window_size\n",
    "        window_end = window_start + window_size\n",
    "        duration = window_end - window_start\n",
    "\n",
    "        n_packets = len(pkts)\n",
    "        total_bytes = sum(pkts)\n",
    "        avg_pkt_size = total_bytes / n_packets\n",
    "        throughput = total_bytes / duration\n",
    "        pkt_rate = n_packets / duration\n",
    "\n",
    "        features.append({\n",
    "            \"window_start\": window_start,\n",
    "            \"window_end\": window_end,\n",
    "            \"packets\": n_packets,\n",
    "            \"bytes\": total_bytes,\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"pkt_rate\": pkt_rate,\n",
    "            \"class\": target_class\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "# ==============================\n",
    "# Procesar todos los archivos\n",
    "# ==============================\n",
    "pcap_files = glob.glob(pcap_pattern)\n",
    "print(f\"🔎 Encontrados {len(pcap_files)} archivos para procesar\")\n",
    "\n",
    "data = []\n",
    "for f in pcap_files:\n",
    "    feats = extract_features_from_pcap_windowed(f, window_size)\n",
    "    data.extend(feats)\n",
    "    print(f\"✅ Procesado {os.path.basename(f)} con {len(feats)} ventanas\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==============================\n",
    "# Guardar dataset\n",
    "# ==============================\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\n✅ Dataset guardado en {output_csv} con {len(df)} flujos\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3566a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Encontrados 1 archivos para procesar\n",
      "✅ Procesado ONU_capture_instant-message_1.pcapng con 274 ventanas\n",
      "\n",
      "✅ Dataset guardado en Dataset/clases/instant-message.csv con 274 flujos\n",
      "        window_start         window_end  packets   bytes  duration  \\\n",
      "0  1693899429.966343  1693899430.966343      497  153868  1.000000   \n",
      "1  1693899430.966343  1693899431.966343      134   86374  1.000000   \n",
      "2  1693899431.966343  1693899432.966343      158   94060  1.000000   \n",
      "3  1693899432.966343  1693899433.966343      301  112154  1.000000   \n",
      "4  1693899433.966343  1693899434.966343      120   28347  1.000000   \n",
      "\n",
      "   avg_pkt_size throughput pkt_rate            class  \n",
      "0    309.593561     153868      497  instant-message  \n",
      "1    644.582090      86374      134  instant-message  \n",
      "2    595.316456   9.406E+4      158  instant-message  \n",
      "3    372.604651     112154      301  instant-message  \n",
      "4    236.225000      28347   1.2E+2  instant-message  \n"
     ]
    }
   ],
   "source": [
    "# clase instant-message\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_instant-message_1.pcapng\"   \n",
    "output_csv = \"Dataset/clases2/instant-message.csv\"\n",
    "target_class = \"instant-message\"\n",
    "window_size = 1  # segundos por ventana\n",
    "\n",
    "# ==============================\n",
    "# Función para extraer features por ventanas\n",
    "# ==============================\n",
    "def extract_features_from_pcap_windowed(pcap_file, window_size=1.0):\n",
    "    packets = rdpcap(pcap_file)\n",
    "\n",
    "    if len(packets) == 0:\n",
    "        return []\n",
    "\n",
    "    start_time = packets[0].time\n",
    "    end_time = packets[-1].time\n",
    "    total_duration = end_time - start_time\n",
    "\n",
    "    n_windows = math.ceil(total_duration / window_size)\n",
    "    windows = [[] for _ in range(n_windows)]\n",
    "\n",
    "    # Distribuir paquetes en ventanas\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            ts = pkt.time\n",
    "            size = len(pkt)\n",
    "            win_idx = int((ts - start_time) // window_size)\n",
    "            if 0 <= win_idx < n_windows:\n",
    "                windows[win_idx].append(size)\n",
    "\n",
    "    # Extraer features de cada ventana\n",
    "    features = []\n",
    "    for i, pkts in enumerate(windows):\n",
    "        if len(pkts) == 0:\n",
    "            continue\n",
    "\n",
    "        window_start = start_time + i * window_size\n",
    "        window_end = window_start + window_size\n",
    "        duration = window_end - window_start\n",
    "\n",
    "        n_packets = len(pkts)\n",
    "        total_bytes = sum(pkts)\n",
    "        avg_pkt_size = total_bytes / n_packets\n",
    "        throughput = total_bytes / duration\n",
    "        pkt_rate = n_packets / duration\n",
    "\n",
    "        features.append({\n",
    "            \"window_start\": window_start,\n",
    "            \"window_end\": window_end,\n",
    "            \"packets\": n_packets,\n",
    "            \"bytes\": total_bytes,\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"pkt_rate\": pkt_rate,\n",
    "            \"class\": target_class\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "# ==============================\n",
    "# Procesar todos los archivos\n",
    "# ==============================\n",
    "pcap_files = glob.glob(pcap_pattern)\n",
    "print(f\"🔎 Encontrados {len(pcap_files)} archivos para procesar\")\n",
    "\n",
    "data = []\n",
    "for f in pcap_files:\n",
    "    feats = extract_features_from_pcap_windowed(f, window_size)\n",
    "    data.extend(feats)\n",
    "    print(f\"✅ Procesado {os.path.basename(f)} con {len(feats)} ventanas\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==============================\n",
    "# Guardar dataset\n",
    "# ==============================\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\n✅ Dataset guardado en {output_csv} con {len(df)} flujos\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64bf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Encontrados 1 archivos para procesar\n",
      "✅ Procesado ONU_capture_mail-service_1.pcapng con 301 ventanas\n",
      "\n",
      "✅ Dataset guardado en Dataset/clases/mail.csv con 301 flujos\n",
      "        window_start         window_end  packets    bytes  duration  \\\n",
      "0  1693898362.004763  1693898363.004763       50     7254  1.000000   \n",
      "1  1693898363.004763  1693898364.004763     4420  2526960  1.000000   \n",
      "2  1693898364.004763  1693898365.004763     3906  2300288  1.000000   \n",
      "3  1693898365.004763  1693898366.004763      841   417849  1.000000   \n",
      "4  1693898366.004763  1693898367.004763       26     6423  1.000000   \n",
      "\n",
      "   avg_pkt_size  throughput pkt_rate class  \n",
      "0    145.080000        7254     5E+1  mail  \n",
      "1    571.710407  2.52696E+6  4.42E+3  mail  \n",
      "2    588.911418     2300288     3906  mail  \n",
      "3    496.847800      417849      841  mail  \n",
      "4    247.038462        6423       26  mail  \n"
     ]
    }
   ],
   "source": [
    "# clase mail\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_mail-service_[1].pcapng\"  \n",
    "output_csv = \"Dataset/clases2/mail.csv\"\n",
    "target_class = \"mail\"\n",
    "window_size = 1  # segundos por ventana\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Función para extraer features por ventanas\n",
    "# ==============================\n",
    "def extract_features_from_pcap_windowed(pcap_file, window_size=1.0):\n",
    "    packets = rdpcap(pcap_file)\n",
    "\n",
    "    if len(packets) == 0:\n",
    "        return []\n",
    "\n",
    "    start_time = packets[0].time\n",
    "    end_time = packets[-1].time\n",
    "    total_duration = end_time - start_time\n",
    "\n",
    "    n_windows = math.ceil(total_duration / window_size)\n",
    "    windows = [[] for _ in range(n_windows)]\n",
    "\n",
    "    # Distribuir paquetes en ventanas\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            ts = pkt.time\n",
    "            size = len(pkt)\n",
    "            win_idx = int((ts - start_time) // window_size)\n",
    "            if 0 <= win_idx < n_windows:\n",
    "                windows[win_idx].append(size)\n",
    "\n",
    "    # Extraer features de cada ventana\n",
    "    features = []\n",
    "    for i, pkts in enumerate(windows):\n",
    "        if len(pkts) == 0:\n",
    "            continue\n",
    "\n",
    "        window_start = start_time + i * window_size\n",
    "        window_end = window_start + window_size\n",
    "        duration = window_end - window_start\n",
    "\n",
    "        n_packets = len(pkts)\n",
    "        total_bytes = sum(pkts)\n",
    "        avg_pkt_size = total_bytes / n_packets\n",
    "        throughput = total_bytes / duration\n",
    "        pkt_rate = n_packets / duration\n",
    "\n",
    "        features.append({\n",
    "            \"window_start\": window_start,\n",
    "            \"window_end\": window_end,\n",
    "            \"packets\": n_packets,\n",
    "            \"bytes\": total_bytes,\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"pkt_rate\": pkt_rate,\n",
    "            \"class\": target_class\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "# ==============================\n",
    "# Procesar todos los archivos\n",
    "# ==============================\n",
    "pcap_files = glob.glob(pcap_pattern)\n",
    "print(f\"🔎 Encontrados {len(pcap_files)} archivos para procesar\")\n",
    "\n",
    "data = []\n",
    "for f in pcap_files:\n",
    "    feats = extract_features_from_pcap_windowed(f, window_size)\n",
    "    data.extend(feats)\n",
    "    print(f\"✅ Procesado {os.path.basename(f)} con {len(feats)} ventanas\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==============================\n",
    "# Guardar dataset\n",
    "# ==============================\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\n✅ Dataset guardado en {output_csv} con {len(df)} flujos\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0d029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Encontrados 3 archivos para procesar\n",
      "✅ Procesado ONU_capture_network-storage_1.pcapng con 303 ventanas\n",
      "✅ Procesado ONU_capture_network-storage_2.pcapng con 279 ventanas\n",
      "✅ Procesado ONU_capture_network-storage_3.pcapng con 302 ventanas\n",
      "\n",
      "✅ Dataset guardado en Dataset/clases/network-storage.csv con 884 flujos\n",
      "        window_start         window_end  packets  bytes  duration  \\\n",
      "0  1693896868.195161  1693896869.195161      265  94732  1.000000   \n",
      "1  1693896869.195161  1693896870.195161      194  91852  1.000000   \n",
      "2  1693896870.195161  1693896871.195161       12   4653  1.000000   \n",
      "3  1693896871.195161  1693896872.195161       25   4638  1.000000   \n",
      "4  1693896872.195161  1693896873.195161       60  18350  1.000000   \n",
      "\n",
      "   avg_pkt_size throughput pkt_rate            class  \n",
      "0    357.479245      94732      265  network-storage  \n",
      "1    473.463918      91852      194  network-storage  \n",
      "2    387.750000       4653       12  network-storage  \n",
      "3    185.520000       4638       25  network-storage  \n",
      "4    305.833333   1.835E+4     6E+1  network-storage  \n"
     ]
    }
   ],
   "source": [
    "# clase network-storage\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_network-storage_[1-3].pcapng\" \n",
    "output_csv = \"Dataset/clases2/network-storage.csv\"\n",
    "target_class = \"network-storage\"\n",
    "window_size = 1  # segundos por ventana\n",
    "\n",
    "# ==============================\n",
    "# Función para extraer features por ventanas\n",
    "# ==============================\n",
    "def extract_features_from_pcap_windowed(pcap_file, window_size=1.0):\n",
    "    packets = rdpcap(pcap_file)\n",
    "\n",
    "    if len(packets) == 0:\n",
    "        return []\n",
    "\n",
    "    start_time = packets[0].time\n",
    "    end_time = packets[-1].time\n",
    "    total_duration = end_time - start_time\n",
    "\n",
    "    n_windows = math.ceil(total_duration / window_size)\n",
    "    windows = [[] for _ in range(n_windows)]\n",
    "\n",
    "    # Distribuir paquetes en ventanas\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            ts = pkt.time\n",
    "            size = len(pkt)\n",
    "            win_idx = int((ts - start_time) // window_size)\n",
    "            if 0 <= win_idx < n_windows:\n",
    "                windows[win_idx].append(size)\n",
    "\n",
    "    # Extraer features de cada ventana\n",
    "    features = []\n",
    "    for i, pkts in enumerate(windows):\n",
    "        if len(pkts) == 0:\n",
    "            continue\n",
    "\n",
    "        window_start = start_time + i * window_size\n",
    "        window_end = window_start + window_size\n",
    "        duration = window_end - window_start\n",
    "\n",
    "        n_packets = len(pkts)\n",
    "        total_bytes = sum(pkts)\n",
    "        avg_pkt_size = total_bytes / n_packets\n",
    "        throughput = total_bytes / duration\n",
    "        pkt_rate = n_packets / duration\n",
    "\n",
    "        features.append({\n",
    "            \"window_start\": window_start,\n",
    "            \"window_end\": window_end,\n",
    "            \"packets\": n_packets,\n",
    "            \"bytes\": total_bytes,\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"pkt_rate\": pkt_rate,\n",
    "            \"class\": target_class\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "# ==============================\n",
    "# Procesar todos los archivos\n",
    "# ==============================\n",
    "pcap_files = glob.glob(pcap_pattern)\n",
    "print(f\"🔎 Encontrados {len(pcap_files)} archivos para procesar\")\n",
    "\n",
    "data = []\n",
    "for f in pcap_files:\n",
    "    feats = extract_features_from_pcap_windowed(f, window_size)\n",
    "    data.extend(feats)\n",
    "    print(f\"✅ Procesado {os.path.basename(f)} con {len(feats)} ventanas\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==============================\n",
    "# Guardar dataset\n",
    "# ==============================\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\n✅ Dataset guardado en {output_csv} con {len(df)} flujos\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ffe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Encontrados 2 archivos para procesar\n",
      "✅ Procesado ONU_capture_video_1.pcapng con 302 ventanas\n",
      "✅ Procesado ONU_capture_video_2.pcapng con 303 ventanas\n",
      "\n",
      "✅ Dataset guardado en Dataset/clases/video.csv con 605 flujos\n",
      "        window_start         window_end  packets   bytes  duration  \\\n",
      "0  1693897322.577452  1693897323.577452     1431  987801  1.000000   \n",
      "1  1693897323.577452  1693897324.577452       46   16015  1.000000   \n",
      "2  1693897324.577452  1693897325.577452       42   14051  1.000000   \n",
      "3  1693897325.577452  1693897326.577452      492  151302  1.000000   \n",
      "4  1693897326.577452  1693897327.577452      379  146994  1.000000   \n",
      "\n",
      "   avg_pkt_size throughput pkt_rate  class  \n",
      "0    690.287212     987801     1431  video  \n",
      "1    348.152174      16015       46  video  \n",
      "2    334.547619      14051       42  video  \n",
      "3    307.524390     151302      492  video  \n",
      "4    387.846966     146994      379  video  \n"
     ]
    }
   ],
   "source": [
    "# clase video\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_video_[1-2].pcapng\" \n",
    "output_csv = \"Dataset/clases2/video.csv\"\n",
    "target_class = \"video\"\n",
    "window_size = 1  # segundos por ventana\n",
    "\n",
    "# ==============================\n",
    "# Función para extraer features por ventanas\n",
    "# ==============================\n",
    "def extract_features_from_pcap_windowed(pcap_file, window_size=1.0):\n",
    "    packets = rdpcap(pcap_file)\n",
    "\n",
    "    if len(packets) == 0:\n",
    "        return []\n",
    "\n",
    "    start_time = packets[0].time\n",
    "    end_time = packets[-1].time\n",
    "    total_duration = end_time - start_time\n",
    "\n",
    "    n_windows = math.ceil(total_duration / window_size)\n",
    "    windows = [[] for _ in range(n_windows)]\n",
    "\n",
    "    # Distribuir paquetes en ventanas\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            ts = pkt.time\n",
    "            size = len(pkt)\n",
    "            win_idx = int((ts - start_time) // window_size)\n",
    "            if 0 <= win_idx < n_windows:\n",
    "                windows[win_idx].append(size)\n",
    "\n",
    "    # Extraer features de cada ventana\n",
    "    features = []\n",
    "    for i, pkts in enumerate(windows):\n",
    "        if len(pkts) == 0:\n",
    "            continue\n",
    "\n",
    "        window_start = start_time + i * window_size\n",
    "        window_end = window_start + window_size\n",
    "        duration = window_end - window_start\n",
    "\n",
    "        n_packets = len(pkts)\n",
    "        total_bytes = sum(pkts)\n",
    "        avg_pkt_size = total_bytes / n_packets\n",
    "        throughput = total_bytes / duration\n",
    "        pkt_rate = n_packets / duration\n",
    "\n",
    "        features.append({\n",
    "            \"window_start\": window_start,\n",
    "            \"window_end\": window_end,\n",
    "            \"packets\": n_packets,\n",
    "            \"bytes\": total_bytes,\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"pkt_rate\": pkt_rate,\n",
    "            \"class\": target_class\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "# ==============================\n",
    "# Procesar todos los archivos\n",
    "# ==============================\n",
    "pcap_files = glob.glob(pcap_pattern)\n",
    "print(f\"🔎 Encontrados {len(pcap_files)} archivos para procesar\")\n",
    "\n",
    "data = []\n",
    "for f in pcap_files:\n",
    "    feats = extract_features_from_pcap_windowed(f, window_size)\n",
    "    data.extend(feats)\n",
    "    print(f\"✅ Procesado {os.path.basename(f)} con {len(feats)} ventanas\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==============================\n",
    "# Guardar dataset\n",
    "# ==============================\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\n✅ Dataset guardado en {output_csv} con {len(df)} flujos\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd8975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Encontrados 2 archivos para procesar\n",
      "✅ Procesado ONU_capture_web-browsing_1.pcapng con 301 ventanas\n",
      "✅ Procesado ONU_capture_web-browsing_2.pcapng con 295 ventanas\n",
      "\n",
      "✅ Dataset guardado en Dataset/clases/web-browsing.csv con 596 flujos\n",
      "        window_start         window_end  packets    bytes  duration  \\\n",
      "0  1693898020.251076  1693898021.251076       30     2308  1.000000   \n",
      "1  1693898021.251076  1693898022.251076       14     7037  1.000000   \n",
      "2  1693898022.251076  1693898023.251076     3004  1527584  1.000000   \n",
      "3  1693898023.251076  1693898024.251076     1691   843539  1.000000   \n",
      "4  1693898024.251076  1693898025.251076     1514   774217  1.000000   \n",
      "\n",
      "   avg_pkt_size throughput pkt_rate         class  \n",
      "0     76.933333       2308     3E+1  web-browsing  \n",
      "1    502.642857       7037       14  web-browsing  \n",
      "2    508.516644    1527584     3004  web-browsing  \n",
      "3    498.840331     843539     1691  web-browsing  \n",
      "4    511.371863     774217     1514  web-browsing  \n"
     ]
    }
   ],
   "source": [
    "# clase web-browsing\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_web-browsing_[1-2].pcapng\" \n",
    "output_csv = \"Dataset/clases2/web-browsing.csv\"\n",
    "target_class = \"web-browsing\"\n",
    "window_size = 1  # segundos por ventana\n",
    "\n",
    "# ==============================\n",
    "# Función para extraer features por ventanas\n",
    "# ==============================\n",
    "def extract_features_from_pcap_windowed(pcap_file, window_size=1.0):\n",
    "    packets = rdpcap(pcap_file)\n",
    "\n",
    "    if len(packets) == 0:\n",
    "        return []\n",
    "\n",
    "    start_time = packets[0].time\n",
    "    end_time = packets[-1].time\n",
    "    total_duration = end_time - start_time\n",
    "\n",
    "    n_windows = math.ceil(total_duration / window_size)\n",
    "    windows = [[] for _ in range(n_windows)]\n",
    "\n",
    "    # Distribuir paquetes en ventanas\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            ts = pkt.time\n",
    "            size = len(pkt)\n",
    "            win_idx = int((ts - start_time) // window_size)\n",
    "            if 0 <= win_idx < n_windows:\n",
    "                windows[win_idx].append(size)\n",
    "\n",
    "    # Extraer features de cada ventana\n",
    "    features = []\n",
    "    for i, pkts in enumerate(windows):\n",
    "        if len(pkts) == 0:\n",
    "            continue\n",
    "\n",
    "        window_start = start_time + i * window_size\n",
    "        window_end = window_start + window_size\n",
    "        duration = window_end - window_start\n",
    "\n",
    "        n_packets = len(pkts)\n",
    "        total_bytes = sum(pkts)\n",
    "        avg_pkt_size = total_bytes / n_packets\n",
    "        throughput = total_bytes / duration\n",
    "        pkt_rate = n_packets / duration\n",
    "\n",
    "        features.append({\n",
    "            \"window_start\": window_start,\n",
    "            \"window_end\": window_end,\n",
    "            \"packets\": n_packets,\n",
    "            \"bytes\": total_bytes,\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"pkt_rate\": pkt_rate,\n",
    "            \"class\": target_class\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "# ==============================\n",
    "# Procesar todos los archivos\n",
    "# ==============================\n",
    "pcap_files = glob.glob(pcap_pattern)\n",
    "print(f\"🔎 Encontrados {len(pcap_files)} archivos para procesar\")\n",
    "\n",
    "data = []\n",
    "for f in pcap_files:\n",
    "    feats = extract_features_from_pcap_windowed(f, window_size)\n",
    "    data.extend(feats)\n",
    "    print(f\"✅ Procesado {os.path.basename(f)} con {len(feats)} ventanas\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==============================\n",
    "# Guardar dataset\n",
    "# ==============================\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\n✅ Dataset guardado en {output_csv} con {len(df)} flujos\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb8465",
   "metadata": {},
   "source": [
    "Codigo General para agrupar dato "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "227bec65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 6 archivos de clases\n",
      "Dataset combinado inicial: 24833 filas\n",
      "✅ Dataset combinado guardado en Dataset/dataset_all1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# ==============================\n",
    "# Configuración de rutas\n",
    "# ==============================\n",
    "clases_dir = \"Dataset/clases1/*.csv\"  # CSVs por clase\n",
    "output_csv = \"Dataset/dataset_all1.csv\"  # Dataset combinado final\n",
    "\n",
    "# ==============================\n",
    "# Cargar todos los CSVs por clase\n",
    "# ==============================\n",
    "csv_files = glob.glob(clases_dir)\n",
    "print(f\"Encontrados {len(csv_files)} archivos de clases\")\n",
    "\n",
    "df_all = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "print(f\"Dataset combinado inicial: {df_all.shape[0]} filas\")\n",
    "\n",
    "# ==============================\n",
    "# Guardar dataset final\n",
    "# ==============================\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df_all.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"✅ Dataset combinado guardado en {output_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58037bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 6 archivos de clases\n",
      "Dataset combinado inicial: 3814 filas\n",
      "✅ Dataset combinado guardado en Dataset/dataset_all2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# ==============================\n",
    "# Configuración de rutas\n",
    "# ==============================\n",
    "clases_dir = \"Dataset/clases2/*.csv\"  # CSVs por clase\n",
    "output_csv = \"Dataset/dataset_all2.csv\"  # Dataset combinado final\n",
    "\n",
    "# ==============================\n",
    "# Cargar todos los CSVs por clase\n",
    "# ==============================\n",
    "csv_files = glob.glob(clases_dir)\n",
    "print(f\"Encontrados {len(csv_files)} archivos de clases\")\n",
    "\n",
    "df_all = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "print(f\"Dataset combinado inicial: {df_all.shape[0]} filas\")\n",
    "\n",
    "# ==============================\n",
    "# Guardar dataset final\n",
    "# ==============================\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df_all.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"✅ Dataset combinado guardado en {output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

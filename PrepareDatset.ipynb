{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4889efb5",
   "metadata": {},
   "source": [
    "Preparación y extracción de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bed87ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion de librerias\n",
    "\n",
    "from scapy.all import rdpcap, TCP, IP\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3941dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset/onu\\ONU_capture_game_1.pcapng → clase game\n",
      "Procesando Dataset/onu\\ONU_capture_game_2.pcapng → clase game\n",
      "Procesando Dataset/onu\\ONU_capture_game_3.pcapng → clase game\n",
      "Procesando Dataset/onu\\ONU_capture_game_4.pcapng → clase game\n",
      "Procesando Dataset/onu\\ONU_capture_game_5.pcapng → clase game\n",
      "Procesando Dataset/onu\\ONU_capture_game_6.pcapng → clase game\n",
      "✅ Guardado 5357 flujos en Dataset/clases/game.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dataset par clase game\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_game_[1-6].pcapng\"  \n",
    "output_csv = \"Dataset/clases/game.csv\"\n",
    "target_class = \"game\"\n",
    "\n",
    "def process_pcap(pcap_file, label):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0, \"bytes\": 0, \"times\": [],\n",
    "        \"syn_count\": 0, \"ack_count\": 0, \"fin_count\": 0, \"rst_count\": 0,\n",
    "        \"pkt_sizes\": []\n",
    "    })\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst\n",
    "            proto = pkt[IP].proto\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1\n",
    "\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        throughput = float(stats[\"bytes\"]) / duration if duration > 0 else float(stats[\"bytes\"])\n",
    "        small_packet_ratio = float(np.sum(np.array(pkt_sizes) < 100)) / float(len(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        pkt_size_var = float(np.var(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        interarrival_var = float(np.var(np.diff(sorted(times_float)))) if len(times_float) > 1 else 0.0\n",
    "        throughput_per_packet = throughput / float(stats[\"packets\"]) if stats[\"packets\"] > 0 else 0.0\n",
    "\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"small_packet_ratio\": small_packet_ratio,\n",
    "            \"pkt_size_var\": pkt_size_var,\n",
    "            \"interarrival_var\": interarrival_var,\n",
    "            \"throughput_per_packet\": throughput_per_packet,\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO ===\n",
    "all_data = []\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))\n",
    "\n",
    "# Guardar CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463d6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset/onu\\ONU_capture_instant-message_1.pcapng → clase instant-message\n",
      "Procesando Dataset/onu\\ONU_capture_instant-message_2.pcapng → clase instant-message\n",
      "Procesando Dataset/onu\\ONU_capture_instant-message_3.pcapng → clase instant-message\n",
      "✅ Guardado 4377 flujos en Dataset/clases/instant-message.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# clase instant-message\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_instant-message_[1-3].pcapng\"   \n",
    "output_csv = \"Dataset/clases/instant-message.csv\"\n",
    "target_class = \"instant-message\"\n",
    "\n",
    "def process_pcap(pcap_file, label):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0, \"bytes\": 0, \"times\": [],\n",
    "        \"syn_count\": 0, \"ack_count\": 0, \"fin_count\": 0, \"rst_count\": 0,\n",
    "        \"pkt_sizes\": []\n",
    "    })\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst\n",
    "            proto = pkt[IP].proto\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1\n",
    "\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        throughput = float(stats[\"bytes\"]) / duration if duration > 0 else float(stats[\"bytes\"])\n",
    "        small_packet_ratio = float(np.sum(np.array(pkt_sizes) < 100)) / float(len(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        pkt_size_var = float(np.var(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        interarrival_var = float(np.var(np.diff(sorted(times_float)))) if len(times_float) > 1 else 0.0\n",
    "        throughput_per_packet = throughput / float(stats[\"packets\"]) if stats[\"packets\"] > 0 else 0.0\n",
    "\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"small_packet_ratio\": small_packet_ratio,\n",
    "            \"pkt_size_var\": pkt_size_var,\n",
    "            \"interarrival_var\": interarrival_var,\n",
    "            \"throughput_per_packet\": throughput_per_packet,\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO ===\n",
    "all_data = []\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))\n",
    "\n",
    "# Guardar CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c42ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset/onu\\ONU_capture_mail-service_1.pcapng → clase mail\n",
      "Procesando Dataset/onu\\ONU_capture_mail-service_2.pcapng → clase mail\n",
      "✅ Guardado 3045 flujos en Dataset/clases/mail.csv\n"
     ]
    }
   ],
   "source": [
    "# clase mail\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_mail-service_[1-2].pcapng\"  \n",
    "output_csv = \"Dataset/clases/mail.csv\"\n",
    "target_class = \"mail\"\n",
    "\n",
    "def process_pcap(pcap_file, label):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0, \"bytes\": 0, \"times\": [],\n",
    "        \"syn_count\": 0, \"ack_count\": 0, \"fin_count\": 0, \"rst_count\": 0,\n",
    "        \"pkt_sizes\": []\n",
    "    })\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst\n",
    "            proto = pkt[IP].proto\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1\n",
    "\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        throughput = float(stats[\"bytes\"]) / duration if duration > 0 else float(stats[\"bytes\"])\n",
    "        small_packet_ratio = float(np.sum(np.array(pkt_sizes) < 100)) / float(len(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        pkt_size_var = float(np.var(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        interarrival_var = float(np.var(np.diff(sorted(times_float)))) if len(times_float) > 1 else 0.0\n",
    "        throughput_per_packet = throughput / float(stats[\"packets\"]) if stats[\"packets\"] > 0 else 0.0\n",
    "\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"small_packet_ratio\": small_packet_ratio,\n",
    "            \"pkt_size_var\": pkt_size_var,\n",
    "            \"interarrival_var\": interarrival_var,\n",
    "            \"throughput_per_packet\": throughput_per_packet,\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO ===\n",
    "all_data = []\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))\n",
    "\n",
    "# Guardar CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc76114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\o'\n",
      "C:\\Users\\Jaime\\AppData\\Local\\Temp\\ipykernel_9392\\2000477355.py:3: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  pcap_pattern = \"Dataset\\onu\\ONU_capture_network-storage_[1-3].pcapng\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset\\onu\\ONU_capture_network-storage_1.pcapng → clase network-storage\n",
      "Procesando Dataset\\onu\\ONU_capture_network-storage_2.pcapng → clase network-storage\n",
      "Procesando Dataset\\onu\\ONU_capture_network-storage_3.pcapng → clase network-storage\n",
      "✅ Guardado 4110 flujos en Dataset/clases/network-storage.csv\n"
     ]
    }
   ],
   "source": [
    "# clase network-storage\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_network-storage_[1-3].pcapng\" \n",
    "output_csv = \"Dataset/clases/network-storage.csv\"\n",
    "target_class = \"network-storage\"\n",
    "\n",
    "def process_pcap(pcap_file, label):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0, \"bytes\": 0, \"times\": [],\n",
    "        \"syn_count\": 0, \"ack_count\": 0, \"fin_count\": 0, \"rst_count\": 0,\n",
    "        \"pkt_sizes\": []\n",
    "    })\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst\n",
    "            proto = pkt[IP].proto\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1\n",
    "\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        throughput = float(stats[\"bytes\"]) / duration if duration > 0 else float(stats[\"bytes\"])\n",
    "        small_packet_ratio = float(np.sum(np.array(pkt_sizes) < 100)) / float(len(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        pkt_size_var = float(np.var(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        interarrival_var = float(np.var(np.diff(sorted(times_float)))) if len(times_float) > 1 else 0.0\n",
    "        throughput_per_packet = throughput / float(stats[\"packets\"]) if stats[\"packets\"] > 0 else 0.0\n",
    "\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"small_packet_ratio\": small_packet_ratio,\n",
    "            \"pkt_size_var\": pkt_size_var,\n",
    "            \"interarrival_var\": interarrival_var,\n",
    "            \"throughput_per_packet\": throughput_per_packet,\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO ===\n",
    "all_data = []\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))\n",
    "\n",
    "# Guardar CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b25f5359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset/onu/ONU_capture_video_1.pcapng → clase video\n",
      "✅ Guardado 3131 flujos en Dataset/clases/video.csv\n"
     ]
    }
   ],
   "source": [
    "# clase network-storage\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_video_1.pcapng\" \n",
    "output_csv = \"Dataset/clases/video.csv\"\n",
    "target_class = \"video\"\n",
    "\n",
    "def process_pcap(pcap_file, label):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0, \"bytes\": 0, \"times\": [],\n",
    "        \"syn_count\": 0, \"ack_count\": 0, \"fin_count\": 0, \"rst_count\": 0,\n",
    "        \"pkt_sizes\": []\n",
    "    })\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst\n",
    "            proto = pkt[IP].proto\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1\n",
    "\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        throughput = float(stats[\"bytes\"]) / duration if duration > 0 else float(stats[\"bytes\"])\n",
    "        small_packet_ratio = float(np.sum(np.array(pkt_sizes) < 100)) / float(len(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        pkt_size_var = float(np.var(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        interarrival_var = float(np.var(np.diff(sorted(times_float)))) if len(times_float) > 1 else 0.0\n",
    "        throughput_per_packet = throughput / float(stats[\"packets\"]) if stats[\"packets\"] > 0 else 0.0\n",
    "\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"small_packet_ratio\": small_packet_ratio,\n",
    "            \"pkt_size_var\": pkt_size_var,\n",
    "            \"interarrival_var\": interarrival_var,\n",
    "            \"throughput_per_packet\": throughput_per_packet,\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO ===\n",
    "all_data = []\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))\n",
    "\n",
    "# Guardar CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e46bfc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Dataset/onu\\ONU_capture_web-browsing_1.pcapng → clase web-browsing\n",
      "Procesando Dataset/onu\\ONU_capture_web-browsing_2.pcapng → clase web-browsing\n",
      "✅ Guardado 4813 flujos en Dataset/clases/web-browsing.csv\n"
     ]
    }
   ],
   "source": [
    "# clase network-storage\n",
    "\n",
    "pcap_pattern = \"Dataset/onu/ONU_capture_web-browsing_[1-2].pcapng\" \n",
    "output_csv = \"Dataset/clases/web-browsing.csv\"\n",
    "target_class = \"web-browsing\"\n",
    "\n",
    "def process_pcap(pcap_file, label):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"packets\": 0, \"bytes\": 0, \"times\": [],\n",
    "        \"syn_count\": 0, \"ack_count\": 0, \"fin_count\": 0, \"rst_count\": 0,\n",
    "        \"pkt_sizes\": []\n",
    "    })\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            src, dst = pkt[IP].src, pkt[IP].dst\n",
    "            proto = pkt[IP].proto\n",
    "            sport = pkt.sport if hasattr(pkt, \"sport\") else 0\n",
    "            dport = pkt.dport if hasattr(pkt, \"dport\") else 0\n",
    "            flow_id = f\"{src}:{sport}-{dst}:{dport}-{proto}\"\n",
    "\n",
    "            flows[flow_id][\"packets\"] += 1\n",
    "            flows[flow_id][\"bytes\"] += len(pkt)\n",
    "            flows[flow_id][\"times\"].append(float(pkt.time))\n",
    "            flows[flow_id][\"pkt_sizes\"].append(len(pkt))\n",
    "\n",
    "            if TCP in pkt:\n",
    "                flags = pkt[TCP].flags\n",
    "                if flags & 0x02: flows[flow_id][\"syn_count\"] += 1\n",
    "                if flags & 0x10: flows[flow_id][\"ack_count\"] += 1\n",
    "                if flags & 0x01: flows[flow_id][\"fin_count\"] += 1\n",
    "                if flags & 0x04: flows[flow_id][\"rst_count\"] += 1\n",
    "\n",
    "    all_data = []\n",
    "    for fid, stats in flows.items():\n",
    "        times_float = stats[\"times\"]\n",
    "        pkt_sizes = stats[\"pkt_sizes\"]\n",
    "\n",
    "        duration = float(max(times_float) - min(times_float)) if len(times_float) > 1 else 0.0\n",
    "        avg_pkt_size = float(np.mean(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        throughput = float(stats[\"bytes\"]) / duration if duration > 0 else float(stats[\"bytes\"])\n",
    "        small_packet_ratio = float(np.sum(np.array(pkt_sizes) < 100)) / float(len(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        pkt_size_var = float(np.var(pkt_sizes)) if pkt_sizes else 0.0\n",
    "        interarrival_var = float(np.var(np.diff(sorted(times_float)))) if len(times_float) > 1 else 0.0\n",
    "        throughput_per_packet = throughput / float(stats[\"packets\"]) if stats[\"packets\"] > 0 else 0.0\n",
    "\n",
    "        all_data.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"packets\": float(stats[\"packets\"]),\n",
    "            \"bytes\": float(stats[\"bytes\"]),\n",
    "            \"duration\": duration,\n",
    "            \"avg_pkt_size\": avg_pkt_size,\n",
    "            \"throughput\": throughput,\n",
    "            \"syn_count\": float(stats[\"syn_count\"]),\n",
    "            \"ack_count\": float(stats[\"ack_count\"]),\n",
    "            \"fin_count\": float(stats[\"fin_count\"]),\n",
    "            \"rst_count\": float(stats[\"rst_count\"]),\n",
    "            \"small_packet_ratio\": small_packet_ratio,\n",
    "            \"pkt_size_var\": pkt_size_var,\n",
    "            \"interarrival_var\": interarrival_var,\n",
    "            \"throughput_per_packet\": throughput_per_packet,\n",
    "            \"class\": label\n",
    "        })\n",
    "    return all_data\n",
    "\n",
    "# === PROCESO ===\n",
    "all_data = []\n",
    "pcap_files = sorted(glob.glob(pcap_pattern))\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Procesando {pcap_file} → clase {target_class}\")\n",
    "    all_data.extend(process_pcap(pcap_file, target_class))\n",
    "\n",
    "# Guardar CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Guardado {len(df)} flujos en {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e671e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 6 archivos de clases\n",
      "✅ Dataset combinado guardado en Dataset/dataset_all.csv\n",
      "Distribución de clases después de combinar:\n",
      "class\n",
      "interactive-app    9734\n",
      "web-browsing       4813\n",
      "network-storage    4110\n",
      "video              3131\n",
      "mail               3045\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Ruta donde están los CSVs por clase\n",
    "clases_dir = \"Dataset/clases/*.csv\"\n",
    "output_csv = \"Dataset/dataset_all.csv\"\n",
    "\n",
    "# Cargar todos los CSVs de clases\n",
    "csv_files = glob.glob(clases_dir)\n",
    "print(f\"Encontrados {len(csv_files)} archivos de clases\")\n",
    "\n",
    "# Unir todos en un solo DataFrame\n",
    "df_all = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "# 🔹 Combinar clases difíciles de separar\n",
    "df_all['class'] = df_all['class'].replace({\n",
    "    'game': 'interactive-app',\n",
    "    'instant-message': 'interactive-app'\n",
    "})\n",
    "\n",
    "# Guardar dataset unificado\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df_all.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"✅ Dataset combinado guardado en {output_csv}\")\n",
    "print(\"Distribución de clases después de combinar:\")\n",
    "print(df_all['class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c982824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de clase minoritaria: 3045\n",
      "\n",
      "=== Distribución de clases tras undersampling ===\n",
      "class\n",
      "web-browsing       3045\n",
      "video              3045\n",
      "interactive-app    3045\n",
      "network-storage    3045\n",
      "mail               3045\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Primeras filas tras escalado ===\n",
      "                                       flow_id   packets     bytes  duration  \\\n",
      "0      124.192.86.248:61079-68.31.89.180:53-17 -0.065342 -0.042128 -0.402400   \n",
      "1   61.139.26.122:41416-113.233.206.8:38191-17 -0.065342 -0.042129 -0.402400   \n",
      "2     61.139.26.122:1433-218.88.239.56:62217-6 -0.064685 -0.042098 -0.392477   \n",
      "3  111.173.128.39:28755-61.139.26.115:58433-17 -0.065342 -0.042072 -0.402400   \n",
      "4   112.50.28.154:27912-61.139.26.115:59957-17 -0.065342 -0.042072 -0.402400   \n",
      "\n",
      "   avg_pkt_size  throughput  syn_count  ack_count  fin_count  rst_count  \\\n",
      "0     -0.496475   -0.036964  -0.673749  -0.056137  -0.609463  -0.076601   \n",
      "1     -0.504662   -0.036964  -0.673749  -0.056137  -0.609463  -0.076601   \n",
      "2     -0.553785   -0.036953  -0.673749  -0.054309  -0.609463   0.327434   \n",
      "3     -0.132143   -0.036958  -0.673749  -0.056137  -0.609463  -0.076601   \n",
      "4     -0.132143   -0.036958  -0.673749  -0.056137  -0.609463  -0.076601   \n",
      "\n",
      "   small_packet_ratio  pkt_size_var  interarrival_var  throughput_per_packet  \\\n",
      "0            0.757709     -0.092547         -0.116259              -0.035987   \n",
      "1            0.757709     -0.092547         -0.116259              -0.035988   \n",
      "2            0.757709     -0.092547         -0.116259              -0.035982   \n",
      "3           -2.230096     -0.092547         -0.116259              -0.035976   \n",
      "4           -2.230096     -0.092547         -0.116259              -0.035976   \n",
      "\n",
      "             class  \n",
      "0     web-browsing  \n",
      "1            video  \n",
      "2     web-browsing  \n",
      "3  interactive-app  \n",
      "4  interactive-app  \n",
      "\n",
      "Dataset balanceado y escalado guardado en 'dataset_balanced_scaled.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv(\"Dataset/dataset_all.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# Balancear clases (undersampling)\n",
    "# -------------------------------\n",
    "min_class_size = df['class'].value_counts().min()\n",
    "print(f\"Tamaño de clase minoritaria: {min_class_size}\")\n",
    "\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "for cls in df['class'].unique():\n",
    "    cls_samples = df[df['class'] == cls].sample(min_class_size, random_state=42)\n",
    "    balanced_df = pd.concat([balanced_df, cls_samples])\n",
    "\n",
    "# Mezclar los datos\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Distribución de clases tras undersampling ===\")\n",
    "print(balanced_df['class'].value_counts())\n",
    "\n",
    "# -------------------------------\n",
    "# Escalar métricas numéricas\n",
    "# -------------------------------\n",
    "features = ['packets','bytes','duration','avg_pkt_size','throughput',\n",
    "            'syn_count','ack_count','fin_count','rst_count',\n",
    "            'small_packet_ratio','pkt_size_var','interarrival_var',\n",
    "            'throughput_per_packet']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "balanced_df[features] = scaler.fit_transform(balanced_df[features])\n",
    "\n",
    "print(\"\\n=== Primeras filas tras escalado ===\")\n",
    "print(balanced_df.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Guardar dataset balanceado y escalado\n",
    "# -------------------------------\n",
    "balanced_df.to_csv(\"Dataset/dataset_balanced_scaled.csv\", index=False)\n",
    "print(\"\\nDataset balanceado y escalado guardado en 'dataset_balanced_scaled.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "766eba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Nuevo dataset guardado en Dataset/dataset_all_features.csv\n",
      "Primeras filas del dataset:\n",
      "                                       flow_id   packets     bytes  duration  \\\n",
      "0      124.192.86.248:61079-68.31.89.180:53-17 -0.065342 -0.042128 -0.402400   \n",
      "1   61.139.26.122:41416-113.233.206.8:38191-17 -0.065342 -0.042129 -0.402400   \n",
      "2     61.139.26.122:1433-218.88.239.56:62217-6 -0.064685 -0.042098 -0.392477   \n",
      "3  111.173.128.39:28755-61.139.26.115:58433-17 -0.065342 -0.042072 -0.402400   \n",
      "4   112.50.28.154:27912-61.139.26.115:59957-17 -0.065342 -0.042072 -0.402400   \n",
      "\n",
      "   avg_pkt_size  throughput  syn_count  ack_count  fin_count  rst_count  ...  \\\n",
      "0     -0.496475   -0.036964  -0.673749  -0.056137  -0.609463  -0.076601  ...   \n",
      "1     -0.504662   -0.036964  -0.673749  -0.056137  -0.609463  -0.076601  ...   \n",
      "2     -0.553785   -0.036953  -0.673749  -0.054309  -0.609463   0.327434  ...   \n",
      "3     -0.132143   -0.036958  -0.673749  -0.056137  -0.609463  -0.076601  ...   \n",
      "4     -0.132143   -0.036958  -0.673749  -0.056137  -0.609463  -0.076601  ...   \n",
      "\n",
      "   control_flag_ratio  bytes_per_packet  throughput_per_avg_pkt  pkt_per_sec  \\\n",
      "0           20.810814          0.644733                0.074453     0.162380   \n",
      "1           20.810814          0.644752                0.073245     0.162380   \n",
      "2           14.775907          0.650811                0.066729     0.164812   \n",
      "3           20.810814          0.643875                0.279685     0.162380   \n",
      "4           20.810814          0.643875                0.279685     0.162380   \n",
      "\n",
      "  bytes_per_sec  log_bytes  log_throughput  log_avg_pkt_size  pkt_bytes_ratio  \\\n",
      "0      0.104692  -0.043041       -0.037664         -0.686121         1.551067   \n",
      "1      0.104695  -0.043042       -0.037664         -0.702515         1.551021   \n",
      "2      0.107262  -0.043009       -0.037653         -0.806955         1.536580   \n",
      "3      0.104553  -0.042983       -0.037659         -0.141729         1.553132   \n",
      "4      0.104553  -0.042983       -0.037659         -0.141729         1.553132   \n",
      "\n",
      "   small_pkt_ratio_per_pkt  \n",
      "0               -11.596296  \n",
      "1               -11.596296  \n",
      "2               -11.714028  \n",
      "3                34.130302  \n",
      "4                34.130302  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "input_csv = \"Dataset/dataset_balanced_scaled.csv\"           # Dataset original\n",
    "output_csv = \"Dataset/dataset_all_features.csv\" # Nuevo dataset con características adicionales\n",
    "\n",
    "# Cargar dataset original\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# --- NUEVAS CARACTERÍSTICAS ---\n",
    "\n",
    "# Ratios de flags TCP\n",
    "df['syn_ratio'] = df['syn_count'] / df['packets']\n",
    "df['ack_ratio'] = df['ack_count'] / df['packets']\n",
    "df['fin_ratio'] = df['fin_count'] / df['packets']\n",
    "df['rst_ratio'] = df['rst_count'] / df['packets']\n",
    "df['control_flag_ratio'] = (df['syn_count'] + df['fin_count'] + df['rst_count']) / df['packets']\n",
    "\n",
    "# Ratios de throughput\n",
    "df['bytes_per_packet'] = df['bytes'] / df['packets']\n",
    "df['throughput_per_avg_pkt'] = df['throughput'] / (df['avg_pkt_size'] + 1e-6)\n",
    "\n",
    "# Paquetes y bytes por segundo\n",
    "df['pkt_per_sec'] = df['packets'] / (df['duration'] + 1e-6)\n",
    "df['bytes_per_sec'] = df['bytes'] / (df['duration'] + 1e-6)\n",
    "\n",
    "# Transformaciones log para reducir skew\n",
    "df['log_bytes'] = np.log1p(df['bytes'])\n",
    "df['log_throughput'] = np.log1p(df['throughput'])\n",
    "df['log_avg_pkt_size'] = np.log1p(df['avg_pkt_size'])\n",
    "\n",
    "# Opcional: interacción de características\n",
    "df['pkt_bytes_ratio'] = df['packets'] / (df['bytes'] + 1e-6)\n",
    "df['small_pkt_ratio_per_pkt'] = df['small_packet_ratio'] / (df['packets'] + 1e-6)\n",
    "\n",
    "# Guardar nuevo CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"✅ Nuevo dataset guardado en {output_csv}\")\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
